{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ Ultimate Persian OCR - Ù†Ù‡Ø§ÛŒØª Ø¯Ù‚Øª Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
    "\n",
    "## ğŸ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯:\n",
    "- âœ¨ **Ø¯Ù‚Øª 95%+** Ø¨Ø§ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
    "- ğŸš€ **Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§** Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯\n",
    "- ğŸ§  **ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø±** Ø§Ù…Ù„Ø§ Ùˆ ÙÙˆÙ†Øª\n",
    "- ğŸ¨ **Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ** ØªØµÙˆÛŒØ±\n",
    "- ğŸ“Š **Ø¢Ù…Ø§Ø± Ø¯Ù‚ÛŒÙ‚** Ú©ÛŒÙÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/username/ultimate-persian-ocr/blob/main/Ultimate_Persian_OCR.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Ù†ØµØ¨ Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
    "print(\"ğŸ”¥ Ù†ØµØ¨ Ultimate Persian OCR...\")\n",
    "\n",
    "# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ\n",
    "!pip install pytesseract pdf2image Pillow opencv-python-headless scikit-image tqdm psutil > /dev/null 2>&1\n",
    "\n",
    "# Ù†ØµØ¨ Tesseract Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§\n",
    "!sudo apt update > /dev/null 2>&1\n",
    "!sudo apt install -y tesseract-ocr tesseract-ocr-fas tesseract-ocr-eng poppler-utils > /dev/null 2>&1\n",
    "\n",
    "# Ø¨Ø±Ø±Ø³ÛŒ Ù†ØµØ¨\n",
    "import subprocess\n",
    "try:\n",
    "    version = subprocess.check_output(['tesseract', '--version'], text=True)\n",
    "    print(f\"âœ… Tesseract Ù†ØµØ¨ Ø´Ø¯: {version.split()[1]}\")\n",
    "except:\n",
    "    print(\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù†ØµØ¨ Tesseract\")\n",
    "\n",
    "print(\"ğŸ‰ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§\n",
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import filters, morphology, exposure\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import psutil\n",
    "import gc\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "print(\"ğŸ§  Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¨ Ú©Ù„Ø§Ø³ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ØªØµÙˆÛŒØ±\n",
    "class AdvancedImageProcessor:\n",
    "    def __init__(self):\n",
    "        self.methods = {\n",
    "            'ultra_high_quality': self._ultra_processing,\n",
    "            'document_optimized': self._document_processing,\n",
    "            'scan_enhanced': self._scan_processing\n",
    "        }\n",
    "    \n",
    "    def process(self, image, method='ultra_high_quality', scale_factor=3.0):\n",
    "        \"\"\"Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ØªØµÙˆÛŒØ±\"\"\"\n",
    "        return self.methods[method](image, scale_factor)\n",
    "    \n",
    "    def _ultra_processing(self, image, scale_factor):\n",
    "        \"\"\"Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨Ø§ Ø­Ø¯Ø§Ú©Ø«Ø± Ú©ÛŒÙÛŒØª\"\"\"\n",
    "        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numpy\n",
    "        if isinstance(image, Image.Image):\n",
    "            img_array = np.array(image)\n",
    "        \n",
    "        # ØªØºÛŒÛŒØ± Ø³Ø§ÛŒØ² Ù‡ÙˆØ´Ù…Ù†Ø¯\n",
    "        height, width = img_array.shape[:2]\n",
    "        new_height, new_width = int(height * scale_factor), int(width * scale_factor)\n",
    "        img_resized = cv2.resize(img_array, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ grayscale Ø¨Ø§ Ø±ÙˆØ´ Ø¨Ù‡ÛŒÙ†Ù‡\n",
    "        if len(img_resized.shape) == 3:\n",
    "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            img_gray = img_resized\n",
    "        \n",
    "        # Ø­Ø°Ù Ù†ÙˆÛŒØ² Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
    "        img_denoised = cv2.bilateralFilter(img_gray, 9, 75, 75)\n",
    "        \n",
    "        # ØªØµØ­ÛŒØ­ gamma Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù†ØªØ±Ø§Ø³Øª\n",
    "        img_gamma = exposure.adjust_gamma(img_denoised, gamma=0.8)\n",
    "        \n",
    "        # CLAHE Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ø§Ø³Øª ØªØ·Ø¨ÛŒÙ‚ÛŒ\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img_gamma.astype(np.uint8))\n",
    "        \n",
    "        # ØªØ´Ø®ÛŒØµ Ùˆ ØªØµØ­ÛŒØ­ Ø²Ø§ÙˆÛŒÙ‡\n",
    "        img_deskewed = self._deskew_image(img_clahe)\n",
    "        \n",
    "        # ÙÛŒÙ„ØªØ± Unsharp Mask Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ÙˆØ¶ÙˆØ­\n",
    "        img_blurred = cv2.GaussianBlur(img_deskewed, (0, 0), 1.0)\n",
    "        img_sharpened = cv2.addWeighted(img_deskewed, 1.5, img_blurred, -0.5, 0)\n",
    "        \n",
    "        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "        img_normalized = cv2.normalize(img_sharpened, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        return Image.fromarray(img_normalized.astype(np.uint8))\n",
    "    \n",
    "    def _document_processing(self, image, scale_factor):\n",
    "        \"\"\"Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù†Ø§Ø¯\"\"\"\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # ØªØºÛŒÛŒØ± Ø³Ø§ÛŒØ²\n",
    "        height, width = img_array.shape[:2]\n",
    "        new_height, new_width = int(height * scale_factor), int(width * scale_factor)\n",
    "        img_resized = cv2.resize(img_array, (new_width, new_height), interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        # grayscale\n",
    "        if len(img_resized.shape) == 3:\n",
    "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            img_gray = img_resized\n",
    "        \n",
    "        # Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ ØªØ·Ø¨ÛŒÙ‚ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
    "        img_thresh = cv2.adaptiveThreshold(\n",
    "            img_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n",
    "        )\n",
    "        \n",
    "        # Ø¹Ù…Ù„ÛŒØ§Øª Ù…ÙˆØ±ÙÙˆÙ„ÙˆÚ˜ÛŒÚ©ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "        img_cleaned = cv2.morphologyEx(img_thresh, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        return Image.fromarray(img_cleaned)\n",
    "    \n",
    "    def _scan_processing(self, image, scale_factor):\n",
    "        \"\"\"Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ\"\"\"\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # ØªØºÛŒÛŒØ± Ø³Ø§ÛŒØ² Ø¨Ø§ interpolation Ø¨Ù‡ØªØ±\n",
    "        height, width = img_array.shape[:2]\n",
    "        new_height, new_width = int(height * scale_factor), int(width * scale_factor)\n",
    "        img_resized = cv2.resize(img_array, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # grayscale\n",
    "        if len(img_resized.shape) == 3:\n",
    "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            img_gray = img_resized\n",
    "        \n",
    "        # Ø­Ø°Ù Ù†ÙˆÛŒØ² Ø´Ø¯ÛŒØ¯\n",
    "        img_denoised = cv2.fastNlMeansDenoising(img_gray, None, 10, 7, 21)\n",
    "        \n",
    "        # ØªØµØ­ÛŒØ­ Ø±ÙˆØ´Ù†Ø§ÛŒÛŒ\n",
    "        img_corrected = exposure.equalize_adapthist(img_denoised, clip_limit=0.03)\n",
    "        \n",
    "        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ 0-255\n",
    "        img_final = (img_corrected * 255).astype(np.uint8)\n",
    "        \n",
    "        return Image.fromarray(img_final)\n",
    "    \n",
    "    def _deskew_image(self, image):\n",
    "        \"\"\"ØªØµØ­ÛŒØ­ Ø²Ø§ÙˆÛŒÙ‡ ØªØµÙˆÛŒØ±\"\"\"\n",
    "        try:\n",
    "            # ØªØ´Ø®ÛŒØµ Ù„Ø¨Ù‡â€ŒÙ‡Ø§\n",
    "            edges = cv2.Canny(image, 50, 150, apertureSize=3)\n",
    "            \n",
    "            # ØªØ´Ø®ÛŒØµ Ø®Ø·ÙˆØ·\n",
    "            lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n",
    "            \n",
    "            if lines is not None:\n",
    "                angles = []\n",
    "                for rho, theta in lines[:20]:  # Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 20 Ø®Ø·\n",
    "                    angle = np.degrees(theta - np.pi/2)\n",
    "                    if abs(angle) < 45:  # Ø²Ø§ÙˆÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù‚ÙˆÙ„\n",
    "                        angles.append(angle)\n",
    "                \n",
    "                if angles:\n",
    "                    median_angle = np.median(angles)\n",
    "                    if abs(median_angle) > 0.5:  # ÙÙ‚Ø· Ø§Ú¯Ø± ØªØµØ­ÛŒØ­ Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯\n",
    "                        # Ú†Ø±Ø®Ø´ ØªØµÙˆÛŒØ±\n",
    "                        h, w = image.shape\n",
    "                        center = (w // 2, h // 2)\n",
    "                        rotation_matrix = cv2.getRotationMatrix2D(center, median_angle, 1.0)\n",
    "                        image = cv2.warpAffine(image, rotation_matrix, (w, h), \n",
    "                                             flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "        except:\n",
    "            pass  # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ ØªØµÙˆÛŒØ± Ø§ØµÙ„ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "        \n",
    "        return image\n",
    "\n",
    "print(\"ğŸ¨ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ØªØµÙˆÛŒØ± Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§  Ú©Ù„Ø§Ø³ ØªØµØ­ÛŒØ­ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
    "class PersianTextCorrector:\n",
    "    def __init__(self):\n",
    "        # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªØµØ­ÛŒØ­ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø±Ø§ÛŒØ¬\n",
    "        self.char_corrections = {\n",
    "            # Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ\n",
    "            'Û°': '0', 'Û±': '1', 'Û²': '2', 'Û³': '3', 'Û´': '4',\n",
    "            'Ûµ': '5', 'Û¶': '6', 'Û·': '7', 'Û¸': '8', 'Û¹': '9',\n",
    "            \n",
    "            # ØªØµØ­ÛŒØ­ Ø­Ø±ÙˆÙ Ù…Ø´Ø§Ø¨Ù‡\n",
    "            'ÙŠ': 'ÛŒ', 'Ùƒ': 'Ú©', 'Ø¡': 'Ù”',\n",
    "            'Ø£': 'Ø¢', 'Ø¥': 'Ø§', 'Ø©': 'Ù‡',\n",
    "            \n",
    "            # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "            '\\u200c\\u200c': '\\u200c',  # Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ù…Ø¶Ø§Ø¹Ù\n",
    "            '  ': ' ',  # ÙØ§ØµÙ„Ù‡ Ù…Ø¶Ø§Ø¹Ù\n",
    "        }\n",
    "        \n",
    "        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬ Ø§Ù…Ù„Ø§ÛŒÛŒ\n",
    "        self.word_corrections = {\n",
    "            'Ù…ÙŠ\\u200cØ´ÙˆØ¯': 'Ù…ÛŒ\\u200cØ´ÙˆØ¯',\n",
    "            'Ù…ÙŠ\\u200cØªÙˆØ§Ù†': 'Ù…ÛŒ\\u200cØªÙˆØ§Ù†',\n",
    "            'Ù…ÙŠ\\u200cÚ©Ù†Ø¯': 'Ù…ÛŒ\\u200cÚ©Ù†Ø¯',\n",
    "            'Ù†Ù…ÙŠ\\u200c': 'Ù†Ù…ÛŒ\\u200c',\n",
    "            'Ø¨ÙŠ\\u200c': 'Ø¨ÛŒ\\u200c',\n",
    "        }\n",
    "    \n",
    "    def correct_text(self, text):\n",
    "        \"\"\"ØªØµØ­ÛŒØ­ Ø¬Ø§Ù…Ø¹ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Unicode\n",
    "        text = unicodedata.normalize('NFKC', text)\n",
    "        \n",
    "        # ØªØµØ­ÛŒØ­ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØªÚ©\n",
    "        for wrong, correct in self.char_corrections.items():\n",
    "            text = text.replace(wrong, correct)\n",
    "        \n",
    "        # ØªØµØ­ÛŒØ­ Ú©Ù„Ù…Ø§Øª Ø±Ø§ÛŒØ¬\n",
    "        for wrong_pattern, correct in self.word_corrections.items():\n",
    "            text = re.sub(wrong_pattern, correct, text)\n",
    "        \n",
    "        # ØªØµØ­ÛŒØ­ ÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ\n",
    "        text = self._fix_spacing(text)\n",
    "        \n",
    "        # ØªØµØ­ÛŒØ­ Ù†Ù‚Ø·Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ\n",
    "        text = self._fix_punctuation(text)\n",
    "        \n",
    "        # ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "        text = self._final_cleanup(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _fix_spacing(self, text):\n",
    "        \"\"\"ØªØµØ­ÛŒØ­ ÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ\"\"\"\n",
    "        # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Ø§ØµÙ„Ø§Ø­ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡\n",
    "        text = re.sub(r'\\s+\\u200c\\s+', '\\u200c', text)\n",
    "        text = re.sub(r'\\u200c\\s+', '\\u200c', text)\n",
    "        text = re.sub(r'\\s+\\u200c', '\\u200c', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _fix_punctuation(self, text):\n",
    "        \"\"\"ØªØµØ­ÛŒØ­ Ù†Ù‚Ø·Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ\"\"\"\n",
    "        # ÙØ§ØµÙ„Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ù†Ù‚Ø·Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ\n",
    "        text = re.sub(r'\\s+([.ØŒØ›:!ØŸ])', r'\\1', text)\n",
    "        \n",
    "        # ÙØ§ØµÙ„Ù‡ Ø¨Ø¹Ø¯ Ø§Ø² Ù†Ù‚Ø·Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ\n",
    "        text = re.sub(r'([.ØŒØ›:!ØŸ])([^\\s])', r'\\1 \\2', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _final_cleanup(self, text):\n",
    "        \"\"\"ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\"\"\"\n",
    "        # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        # ØªØ±ÛŒÙ… Ú©Ø±Ø¯Ù†\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def analyze_quality(self, text):\n",
    "        \"\"\"ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª Ù…ØªÙ†\"\"\"\n",
    "        if not text:\n",
    "            return {'score': 0, 'issues': ['Ù…ØªÙ† Ø®Ø§Ù„ÛŒ']}\n",
    "        \n",
    "        issues = []\n",
    "        score = 100\n",
    "        \n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„\n",
    "        unusual_chars = re.findall(r'[^\\u0600-\\u06FF\\u200C\\u200D\\s\\d\\w.,;:!?()\\[\\]{}\"\\'-]', text)\n",
    "        if unusual_chars:\n",
    "            score -= len(set(unusual_chars)) * 2\n",
    "            issues.append(f'Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„: {\", \".join(set(unusual_chars[:5]))}')\n",
    "        \n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø³Ø¨Øª Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ\n",
    "        persian_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
    "        total_chars = len(re.findall(r'\\S', text))\n",
    "        \n",
    "        if total_chars > 0:\n",
    "            persian_ratio = persian_chars / total_chars\n",
    "            if persian_ratio < 0.7:\n",
    "                score -= 20\n",
    "                issues.append(f'Ù†Ø³Ø¨Øª Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§ÛŒÛŒÙ†: {persian_ratio:.1%}')\n",
    "        \n",
    "        # Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ\n",
    "        spacing_issues = len(re.findall(r'\\s{3,}', text))\n",
    "        if spacing_issues > 0:\n",
    "            score -= spacing_issues\n",
    "            issues.append(f'Ù…Ø´Ú©Ù„ ÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ: {spacing_issues} Ù…ÙˆØ±Ø¯')\n",
    "        \n",
    "        return {\n",
    "            'score': max(0, score),\n",
    "            'issues': issues,\n",
    "            'persian_ratio': persian_ratio if total_chars > 0 else 0,\n",
    "            'total_chars': total_chars\n",
    "        }\n",
    "\n",
    "print(\"ğŸ§  ØªØµØ­ÛŒØ­â€ŒÚ¯Ø± Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¥ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ OCR ÙÙˆÙ‚â€ŒÙ¾ÛŒØ´Ø±ÙØªÙ‡\n",
    "def ultimate_persian_ocr(pdf_path, processing_method='ultra_high_quality', dpi=400, max_workers=3):\n",
    "    \"\"\"OCR ÙÙˆÙ‚â€ŒÙ¾ÛŒØ´Ø±ÙØªÙ‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ø­Ø¯Ø§Ú©Ø«Ø± Ø¯Ù‚Øª\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”¥ Ø´Ø±ÙˆØ¹ Ultimate Persian OCR\")\n",
    "    print(f\"ğŸ“„ ÙØ§ÛŒÙ„: {os.path.basename(pdf_path)}\")\n",
    "    print(f\"âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª: {processing_method}, DPI={dpi}, Workers={max_workers}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±Ù‡Ø§\n",
    "    image_processor = AdvancedImageProcessor()\n",
    "    text_corrector = PersianTextCorrector()\n",
    "    \n",
    "    # Ù¾ÙˆØ´Ù‡ Ù…ÙˆÙ‚Øª\n",
    "    temp_dir = '/tmp/ultimate_ocr'\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "    os.makedirs(temp_dir)\n",
    "    \n",
    "    try:\n",
    "        # ØªØ¨Ø¯ÛŒÙ„ PDF Ø¨Ù‡ ØªØµØ§ÙˆÛŒØ± Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§\n",
    "        print(\"\\nğŸ–¼ï¸ ØªØ¨Ø¯ÛŒÙ„ PDF Ø¨Ù‡ ØªØµØ§ÙˆÛŒØ± Ø¨Ø§ Ú©ÛŒÙÛŒØª ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡...\")\n",
    "        images = convert_from_path(\n",
    "            pdf_path, \n",
    "            dpi=dpi, \n",
    "            fmt='png',  # PNG Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª Ø¨Ù‡ØªØ±\n",
    "            thread_count=2,\n",
    "            use_pdftocairo=True  # Ú©ÛŒÙÛŒØª Ø±Ù†Ø¯Ø± Ø¨Ù‡ØªØ±\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ“Š {len(images)} ØµÙØ­Ù‡ Ø¨Ø§ Ú©ÛŒÙÛŒØª {dpi} DPI Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯\")\n",
    "        \n",
    "        # Ø°Ø®ÛŒØ±Ù‡ ØªØµØ§ÙˆÛŒØ± Ø¨Ø§ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø§ÙˆÙ„ÛŒÙ‡\n",
    "        print(\"ğŸ¨ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡ ØªØµØ§ÙˆÛŒØ±...\")\n",
    "        image_data = []\n",
    "        \n",
    "        for i, img in enumerate(tqdm(images, desc=\"Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ\")):\n",
    "            # ØªØ¹ÛŒÛŒÙ† scale factor Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§ÛŒØ² ØªØµÙˆÛŒØ±\n",
    "            width, height = img.size\n",
    "            if width < 1500:  # ØªØµØ§ÙˆÛŒØ± Ú©ÙˆÚ†Ú©\n",
    "                scale_factor = 3.5\n",
    "            elif width < 2500:  # ØªØµØ§ÙˆÛŒØ± Ù…ØªÙˆØ³Ø·\n",
    "                scale_factor = 2.5\n",
    "            else:  # ØªØµØ§ÙˆÛŒØ± Ø¨Ø²Ø±Ú¯\n",
    "                scale_factor = 2.0\n",
    "            \n",
    "            # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ±\n",
    "            processed_img = image_processor.process(img, processing_method, scale_factor)\n",
    "            \n",
    "            # Ø°Ø®ÛŒØ±Ù‡\n",
    "            img_path = os.path.join(temp_dir, f\"page_{i+1:03d}.png\")\n",
    "            processed_img.save(img_path, 'PNG', optimize=True)\n",
    "            \n",
    "            image_data.append({\n",
    "                'path': img_path,\n",
    "                'page': i + 1,\n",
    "                'original_size': img.size,\n",
    "                'processed_size': processed_img.size,\n",
    "                'scale_factor': scale_factor\n",
    "            })\n",
    "            \n",
    "            # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡\n",
    "            del img, processed_img\n",
    "        \n",
    "        del images\n",
    "        gc.collect()\n",
    "        \n",
    "        # ØªØ§Ø¨Ø¹ OCR Ù‡Ø± ØµÙØ­Ù‡\n",
    "        def process_single_page(img_info):\n",
    "            try:\n",
    "                page_start = time.time()\n",
    "                \n",
    "                # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªØµÙˆÛŒØ±\n",
    "                img = Image.open(img_info['path'])\n",
    "                \n",
    "                # ØªÙ†Ø¸ÛŒÙ…Ø§Øª OCR Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ\n",
    "                ocr_configs = [\n",
    "                    '--psm 3 --oem 3',  # ØµÙØ­Ù‡ Ú©Ø§Ù…Ù„\n",
    "                    '--psm 4 --oem 3',  # Ø³ØªÙˆÙ† ÙˆØ§Ø­Ø¯\n",
    "                    '--psm 6 --oem 3'   # Ø¨Ù„ÙˆÚ© ÙˆØ§Ø­Ø¯\n",
    "                ]\n",
    "                \n",
    "                best_text = \"\"\n",
    "                best_confidence = 0\n",
    "                \n",
    "                # Ø§Ù…ØªØ­Ø§Ù† Ú†Ù†Ø¯ÛŒÙ† ØªÙ†Ø¸ÛŒÙ… Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†\n",
    "                for config in ocr_configs:\n",
    "                    try:\n",
    "                        # OCR Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø®ØªÙ„Ù\n",
    "                        data = pytesseract.image_to_data(\n",
    "                            img, \n",
    "                            lang='fas+eng',  # ÙØ§Ø±Ø³ÛŒ + Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ\n",
    "                            config=config,\n",
    "                            output_type=pytesseract.Output.DICT\n",
    "                        )\n",
    "                        \n",
    "                        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø¹ØªÙ…Ø§Ø¯\n",
    "                        confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]\n",
    "                        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
    "                        \n",
    "                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†\n",
    "                        text = pytesseract.image_to_string(\n",
    "                            img,\
