{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ğŸ”¥ Ultimate Persian OCR - Ù†Ù‡Ø§ÛŒØª Ø¯Ù‚Øª Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MohammadHNdev/ultimate-persian-ocr/blob/main/Ultimate_Persian_OCR.ipynb)\n",
        "\n",
        "## ğŸ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯:\n",
        "- âœ¨ **Ø¯Ù‚Øª 95%+** Ø¨Ø§ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
        "- ğŸš€ **Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§** Ø¨Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯\n",
        "- ğŸ§  **ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø±** Ø§Ù…Ù„Ø§ Ùˆ ÙÙˆÙ†Øª\n",
        "- ğŸ¨ **Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ** ØªØµÙˆÛŒØ±\n",
        "- ğŸ“Š **Ø¢Ù…Ø§Ø± Ø¯Ù‚ÛŒÙ‚** Ú©ÛŒÙÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸš€ Ù†ØµØ¨ Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
        "print(\"ğŸ”¥ Ù†ØµØ¨ Ultimate Persian OCR...\")\n",
        "\n",
        "# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ\n",
        "!pip install pytesseract pdf2image Pillow opencv-python-headless scikit-image tqdm psutil > /dev/null 2>&1\n",
        "\n",
        "# Ù†ØµØ¨ Tesseract Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§\n",
        "!sudo apt update > /dev/null 2>&1\n",
        "!sudo apt install -y tesseract-ocr tesseract-ocr-fas tesseract-ocr-eng poppler-utils > /dev/null 2>&1\n",
        "\n",
        "print(\"ğŸ‰ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯!\")"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“š ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§\n",
        "import pytesseract\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ImageOps\n",
        "from pdf2image import convert_from_path\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import filters, morphology, exposure\n",
        "import os, time, re, unicodedata, shutil, psutil, gc, json\n",
        "from tqdm.notebook import tqdm\n",
        "import concurrent.futures\n",
        "from google.colab import files\n",
        "from collections import Counter\n",
        "\n",
        "print(\"ğŸ§  Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ¨ Ú©Ù„Ø§Ø³ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ØªØµÙˆÛŒØ±\n",
        "class AdvancedImageProcessor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def process(self, image, scale_factor=3.0):\n",
        "        \"\"\"Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø¨Ø§ Ø­Ø¯Ø§Ú©Ø«Ø± Ú©ÛŒÙÛŒØª\"\"\"\n",
        "        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numpy\n",
        "        img_array = np.array(image)\n",
        "        \n",
        "        # ØªØºÛŒÛŒØ± Ø³Ø§ÛŒØ² Ù‡ÙˆØ´Ù…Ù†Ø¯\n",
        "        height, width = img_array.shape[:2]\n",
        "        new_height, new_width = int(height * scale_factor), int(width * scale_factor)\n",
        "        img_resized = cv2.resize(img_array, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
        "        \n",
        "        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ grayscale\n",
        "        if len(img_resized.shape) == 3:\n",
        "            img_gray = cv2.cvtColor(img_resized, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            img_gray = img_resized\n",
        "        \n",
        "        # Ø­Ø°Ù Ù†ÙˆÛŒØ² Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
        "        img_denoised = cv2.bilateralFilter(img_gray, 9, 75, 75)\n",
        "        \n",
        "        # ØªØµØ­ÛŒØ­ gamma\n",
        "        img_gamma = exposure.adjust_gamma(img_denoised, gamma=0.8)\n",
        "        \n",
        "        # CLAHE Ø¨Ø±Ø§ÛŒ Ú©Ù†ØªØ±Ø§Ø³Øª ØªØ·Ø¨ÛŒÙ‚ÛŒ\n",
        "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "        img_clahe = clahe.apply(img_gamma.astype(np.uint8))\n",
        "        \n",
        "        # ÙÛŒÙ„ØªØ± Unsharp Mask\n",
        "        img_blurred = cv2.GaussianBlur(img_clahe, (0, 0), 1.0)\n",
        "        img_sharpened = cv2.addWeighted(img_clahe, 1.5, img_blurred, -0.5, 0)\n",
        "        \n",
        "        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ\n",
        "        img_normalized = cv2.normalize(img_sharpened, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        \n",
        "        return Image.fromarray(img_normalized.astype(np.uint8))\n",
        "\n",
        "print(\"ğŸ¨ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± ØªØµÙˆÛŒØ± Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯!\")"
      ],
      "metadata": {
        "id": "image_processor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§  Ú©Ù„Ø§Ø³ ØªØµØ­ÛŒØ­ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
        "class PersianTextCorrector:\n",
        "    def __init__(self):\n",
        "        # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªØµØ­ÛŒØ­ Ú©Ø§Ø±Ø§Ú©ØªØ±\n",
        "        self.char_corrections = {\n",
        "            'Û°': '0', 'Û±': '1', 'Û²': '2', 'Û³': '3', 'Û´': '4',\n",
        "            'Ûµ': '5', 'Û¶': '6', 'Û·': '7', 'Û¸': '8', 'Û¹': '9',\n",
        "            'ÙŠ': 'ÛŒ', 'Ùƒ': 'Ú©', 'Ø¡': 'Ù”',\n",
        "            'Ø£': 'Ø¢', 'Ø¥': 'Ø§', 'Ø©': 'Ù‡',\n",
        "            '  ': ' '  # ÙØ§ØµÙ„Ù‡ Ù…Ø¶Ø§Ø¹Ù\n",
        "        }\n",
        "    \n",
        "    def correct_text(self, text):\n",
        "        \"\"\"ØªØµØ­ÛŒØ­ Ø¬Ø§Ù…Ø¹ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\"\"\"\n",
        "        if not text:\n",
        "            return text\n",
        "        \n",
        "        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Unicode\n",
        "        text = unicodedata.normalize('NFKC', text)\n",
        "        \n",
        "        # ØªØµØ­ÛŒØ­ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§\n",
        "        for wrong, correct in self.char_corrections.items():\n",
        "            text = text.replace(wrong, correct)\n",
        "        \n",
        "        # ØªØµØ­ÛŒØ­ ÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'\\s+([.ØŒØ›:!ØŸ])', r'\\1', text)\n",
        "        text = re.sub(r'([.ØŒØ›:!ØŸ])([^\\s])', r'\\1 \\2', text)\n",
        "        \n",
        "        # ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "        text = text.strip()\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    def analyze_quality(self, text):\n",
        "        \"\"\"ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª Ù…ØªÙ†\"\"\"\n",
        "        if not text:\n",
        "            return {'score': 0, 'issues': ['Ù…ØªÙ† Ø®Ø§Ù„ÛŒ']}\n",
        "        \n",
        "        score = 100\n",
        "        issues = []\n",
        "        \n",
        "        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„\n",
        "        unusual_chars = re.findall(r'[^\\u0600-\\u06FF\\u200C\\u200D\\s\\d\\w.,;:!?()\\[\\]{}\"\\'-]', text)\n",
        "        if unusual_chars:\n",
        "            score -= len(set(unusual_chars)) * 2\n",
        "            issues.append(f'Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„: {\", \".join(set(unusual_chars[:3]))}')\n",
        "        \n",
        "        # Ù†Ø³Ø¨Øª Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ\n",
        "        persian_chars = len(re.findall(r'[\\u0600-\\u06FF]', text))\n",
        "        total_chars = len(re.findall(r'\\S', text))\n",
        "        persian_ratio = persian_chars / total_chars if total_chars > 0 else 0\n",
        "        \n",
        "        if persian_ratio < 0.7:\n",
        "            score -= 20\n",
        "            issues.append(f'Ù†Ø³Ø¨Øª Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§ÛŒÛŒÙ†: {persian_ratio:.1%}')\n",
        "        \n",
        "        return {\n",
        "            'score': max(0, score),\n",
        "            'issues': issues,\n",
        "            'persian_ratio': persian_ratio,\n",
        "            'total_chars': total_chars\n",
        "        }\n",
        "\n",
        "print(\"ğŸ§  ØªØµØ­ÛŒØ­â€ŒÚ¯Ø± Ù…ØªÙ† Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯!\")"
      ],
      "metadata": {
        "id": "text_corrector"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¥ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ultimate OCR\n",
        "def ultimate_persian_ocr(pdf_path, dpi=400, max_workers=3):\n",
        "    \"\"\"OCR ÙÙˆÙ‚â€ŒÙ¾ÛŒØ´Ø±ÙØªÙ‡ ÙØ§Ø±Ø³ÛŒ\"\"\"\n",
        "    \n",
        "    print(f\"ğŸ”¥ Ø´Ø±ÙˆØ¹ Ultimate Persian OCR: {os.path.basename(pdf_path)}\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø±Ù‡Ø§\n",
        "    image_processor = AdvancedImageProcessor()\n",
        "    text_corrector = PersianTextCorrector()\n",
        "    \n",
        "    # Ù¾ÙˆØ´Ù‡ Ù…ÙˆÙ‚Øª\n",
        "    temp_dir = '/tmp/ultimate_ocr'\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "    os.makedirs(temp_dir)\n",
        "    \n",
        "    try:\n",
        "        # ØªØ¨Ø¯ÛŒÙ„ PDF Ø¨Ù‡ ØªØµØ§ÙˆÛŒØ±\n",
        "        print(\"ğŸ–¼ï¸ ØªØ¨Ø¯ÛŒÙ„ PDF Ø¨Ù‡ ØªØµØ§ÙˆÛŒØ±...\")\n",
        "        images = convert_from_path(pdf_path, dpi=dpi, fmt='png')\n",
        "        print(f\"ğŸ“Š {len(images)} ØµÙØ­Ù‡ Ø¢Ù…Ø§Ø¯Ù‡\")\n",
        "        \n",
        "        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØµØ§ÙˆÛŒØ±\n",
        "        image_data = []\n",
        "        for i, img in enumerate(tqdm(images, desc=\"Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´\")):\n",
        "            # ØªØ¹ÛŒÛŒÙ† scale factor\n",
        "            width, height = img.size\n",
        "            scale_factor = 3.5 if width < 1500 else 2.5 if width < 2500 else 2.0\n",
        "            \n",
        "            # Ù¾Ø±Ø¯Ø§Ø²Ø´\n",
        "            processed_img = image_processor.process(img, scale_factor)\n",
        "            \n",
        "            # Ø°Ø®ÛŒØ±Ù‡\n",
        "            img_path = os.path.join(temp_dir, f\"page_{i+1:03d}.png\")\n",
        "            processed_img.save(img_path, 'PNG')\n",
        "            \n",
        "            image_data.append({\n",
        "                'path': img_path,\n",
        "                'page': i + 1,\n",
        "                'scale_factor': scale_factor\n",
        "            })\n",
        "            \n",
        "            del img, processed_img\n",
        "        \n",
        "        del images\n",
        "        gc.collect()\n",
        "        \n",
        "        # ØªØ§Ø¨Ø¹ OCR Ù‡Ø± ØµÙØ­Ù‡\n",
        "        def process_page(img_info):\n",
        "            try:\n",
        "                img = Image.open(img_info['path'])\n",
        "                \n",
        "                # OCR Ø¨Ø§ Ú†Ù†Ø¯ ØªÙ†Ø¸ÛŒÙ… Ù…Ø®ØªÙ„Ù\n",
        "                configs = ['--psm 3 --oem 3', '--psm 4 --oem 3', '--psm 6 --oem 3']\n",
        "                best_text = \"\"\n",
        "                best_confidence = 0\n",
        "                \n",
        "                for config in configs:\n",
        "                    try:\n",
        "                        # Ù…Ø­Ø§Ø³Ø¨Ù‡ confidence\n",
        "                        data = pytesseract.image_to_data(\n",
        "                            img, lang='fas+eng', config=config, output_type=pytesseract.Output.DICT\n",
        "                        )\n",
        "                        confidences = [int(c) for c in data['conf'] if int(c) > 0]\n",
        "                        avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
        "                        \n",
        "                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†\n",
        "                        text = pytesseract.image_to_string(img, lang='fas+eng', config=config)\n",
        "                        \n",
        "                        if avg_confidence > best_confidence and len(text.strip()) > len(best_text.strip()):\n",
        "                            best_text = text\n",
        "                            best_confidence = avg_confidence\n",
        "                    except:\n",
        "                        continue\n",
        "                \n",
        "                # ØªØµØ­ÛŒØ­ Ù…ØªÙ†\n",
        "                corrected_text = text_corrector.correct_text(best_text)\n",
        "                quality = text_corrector.analyze_quality(corrected_text)\n",
        "                \n",
        "                del img\n",
        "                os.remove(img_info['path'])\n",
        "                \n",
        "                return {\n",
        "                    'page': img_info['page'],\n",
        "                    'text': corrected_text,\n",
        "                    'confidence': best_confidence,\n",
        "                    'quality': quality,\n",
        "                    'char_count': len(corrected_text),\n",
        "                    'success': True\n",
        "                }\n",
        "            except Exception as e:\n",
        "                return {\n",
        "                    'page': img_info['page'],\n",
        "                    'text': '',\n",
        "                    'confidence': 0,\n",
        "                    'quality': {'score': 0, 'issues': [str(e)]},\n",
        "                    'char_count': 0,\n",
        "                    'success': False\n",
        "                }\n",
        "        \n",
        "        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ\n",
        "        print(f\"ğŸ§  Ø´Ø±ÙˆØ¹ OCR Ø¨Ø§ {max_workers} worker...\")\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            results = list(tqdm(\n",
        "                executor.map(process_page, image_data),\n",
        "                total=len(image_data),\n",
        "                desc=\"OCR ØµÙØ­Ø§Øª\"\n",
        "            ))\n",
        "        \n",
        "        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ\n",
        "        results.sort(key=lambda x: x['page'])\n",
        "        \n",
        "        # Ø¢Ù…Ø§Ø±\n",
        "        successful = [r for r in results if r['success']]\n",
        "        total_chars = sum(r['char_count'] for r in successful)\n",
        "        avg_confidence = sum(r['confidence'] for r in successful) / len(successful) if successful else 0\n",
        "        avg_quality = sum(r['quality']['score'] for r in successful) / len(successful) if successful else 0\n",
        "        \n",
        "        # Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ\n",
        "        output_filename = f\"ultimate_ocr_{int(time.time())}.txt\"\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"# ğŸ”¥ Ultimate Persian OCR Results\\n\")\n",
        "            f.write(f\"# File: {os.path.basename(pdf_path)}\\n\")\n",
        "            f.write(f\"# Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "            \n",
        "            for result in results:\n",
        "                f.write(f\"\\n--- ØµÙØ­Ù‡ {result['page']} ---\\n\")\n",
        "                if result['success']:\n",
        "                    f.write(f\"Ø§Ø¹ØªÙ…Ø§Ø¯: {result['confidence']:.1f}% | Ú©ÛŒÙÛŒØª: {result['quality']['score']:.1f}\\n\")\n",
        "                    f.write(result['text'])\n",
        "                else:\n",
        "                    f.write(\"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´\")\n",
        "                f.write(\"\\n\\n\")\n",
        "        \n",
        "        # Ù†ØªØ§ÛŒØ¬\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\nğŸ‰ OCR Ú©Ø§Ù…Ù„ Ø´Ø¯!\")\n",
        "        print(f\"ğŸ“Š ØµÙØ­Ø§Øª Ù…ÙˆÙÙ‚: {len(successful)}/{len(results)}\")\n",
        "        print(f\"ğŸ“ Ú©Ù„ Ú©Ø§Ø±Ø§Ú©ØªØ±: {total_chars:,}\")\n",
        "        print(f\"ğŸ¯ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø¹ØªÙ…Ø§Ø¯: {avg_confidence:.1f}%\")\n",
        "        print(f\"â­ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©ÛŒÙÛŒØª: {avg_quality:.1f}/100\")\n",
        "        print(f\"â±ï¸ Ø²Ù…Ø§Ù†: {total_time:.1f}s\")\n",
        "        print(f\"ğŸ“ ÙØ§ÛŒÙ„: {output_filename}\")\n",
        "        \n",
        "        return {\n",
        "            'success': True,\n",
        "            'output_file': output_filename,\n",
        "            'stats': {\n",
        "                'total_pages': len(results),\n",
        "                'successful_pages': len(successful),\n",
        "                'total_characters': total_chars,\n",
        "                'avg_confidence': avg_confidence,\n",
        "                'avg_quality': avg_quality,\n",
        "                'processing_time': total_time\n",
        "            }\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø§: {e}\")\n",
        "        return {'success': False, 'error': str(e)}\n",
        "    finally:\n",
        "        if os.path.exists(temp_dir):\n",
        "            shutil.rmtree(temp_dir)\n",
        "        gc.collect()\n",
        "\n",
        "print(\"ğŸ”¥ Ultimate OCR Ø¢Ù…Ø§Ø¯Ù‡!\")"
      ],
      "metadata": {
        "id": "main_ocr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¤ Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„\n",
        "print(\"ğŸ“¤ Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ PDF Ø®ÙˆØ¯ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    pdf_file = None\n",
        "    for filename in uploaded.keys():\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            pdf_file = filename\n",
        "            break\n",
        "    \n",
        "    if pdf_file:\n",
        "        file_size_mb = len(uploaded[pdf_file]) / (1024*1024)\n",
        "        print(f\"âœ… ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯: {pdf_file} ({file_size_mb:.1f} MB)\")\n",
        "        \n",
        "        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡\n",
        "        if file_size_mb < 10:\n",
        "            dpi, workers = 450, 3\n",
        "        elif file_size_mb < 30:\n",
        "            dpi, workers = 400, 3\n",
        "        else:\n",
        "            dpi, workers = 350, 2\n",
        "        \n",
        "        print(f\"âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡: DPI={dpi}, Workers={workers}\")\n",
        "    else:\n",
        "        print(\"âŒ ÙØ§ÛŒÙ„ PDF ÛŒØ§ÙØª Ù†Ø´Ø¯!\")\n",
        "        pdf_file = None\nelse:\n    print(\"âŒ ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯!\")\n    pdf_file = None"
      ],
      "metadata": {
        "id": "upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸš€ Ø§Ø¬Ø±Ø§ÛŒ Ultimate OCR\n",
        "if 'pdf_file' in locals() and pdf_file:\n",
        "    print(f\"ğŸ”¥ Ø´Ø±ÙˆØ¹ Ultimate Persian OCR Ø¨Ø±Ø§ÛŒ {pdf_file}\")\n",
        "    print(\"â˜• ØµØ¨Ø± Ú©Ù†ÛŒØ¯ØŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒØªØ§Ù† ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…...\")\n",
        "    \n",
        "    result = ultimate_persian_ocr(pdf_file, dpi, workers)\n",
        "    \n",
        "    if result['success']:\n",
        "        print(f\"\\nğŸ† ØªØ¨Ø±ÛŒÚ©! OCR Ø¨Ø§ Ú©ÛŒÙÛŒØª ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!\")\n",
        "        \n",
        "        stats = result['stats']\n",
        "        success_rate = stats['successful_pages'] / stats['total_pages'] * 100\n",
        "        \n",
        "        print(f\"ğŸ“ˆ Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ:\")\n",
        "        print(f\"   ğŸ“„ ØµÙØ­Ø§Øª: {stats['successful_pages']}/{stats['total_pages']} ({success_rate:.1f}%)\")\n",
        "        print(f\"   ğŸ“ Ú©Ø§Ø±Ø§Ú©ØªØ±: {stats['total_characters']:,}\")\n",
        "        print(f\"   ğŸ¯ Ø§Ø¹ØªÙ…Ø§Ø¯: {stats['avg_confidence']:.1f}%\")\n",
        "        print(f\"   â­ Ú©ÛŒÙÛŒØª: {stats['avg_quality']:.1f}/100\")\n",
        "        print(f\"   â±ï¸ Ø²Ù…Ø§Ù†: {stats['processing_time']:.1f}s\")\n",
        "        \n",
        "        # Ø±ØªØ¨Ù‡ Ú©ÛŒÙÛŒØª\n",
        "        if stats['avg_quality'] >= 90:\n",
        "            grade = \"ğŸ† Ø¹Ø§Ù„ÛŒ\"\n",
        "        elif stats['avg_quality'] >= 75:\n",
        "            grade = \"ğŸ¥‡ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨\"\n",
        "        elif stats['avg_quality'] >= 60:\n",
        "            grade = \"ğŸ¥ˆ Ø®ÙˆØ¨\"\n",
        "        else:\n",
        "            grade = \"ğŸ¥‰ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„\"\n",
        "        \n",
        "        print(f\"   ğŸ… Ø±ØªØ¨Ù‡: {grade}\")\n",
        "        \n",
        "        # Ø¯Ø§Ù†Ù„ÙˆØ¯\n",
        "        try:\n",
        "            files.download(result['output_file'])\n",
        "            print(f\"ğŸ’¾ ÙØ§ÛŒÙ„ {result['output_file']} Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯!\")\n",
        "        except:\n",
        "            print(f\"ğŸ“ ÙØ§ÛŒÙ„ Ø¯Ø± Ù¾Ù†Ù„ Colab: {result['output_file']}\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"âŒ Ø®Ø·Ø§: {result.get('error', 'Ù†Ø§Ù…Ø´Ø®Øµ')}\")\n",
        "else:\n",
        "    print(\"âŒ Ø§Ø¨ØªØ¯Ø§ ÙØ§ÛŒÙ„ PDF Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯!\")"
      ],
      "metadata": {
        "id": "run_ocr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "---\n",
        "\n",
        "## ğŸ‰ Ultimate Persian OCR Ú©Ø§Ù…Ù„ Ø´Ø¯!\n",
        "\n",
        "### ğŸ”¥ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø±:\n",
        "\n",
        "- **ğŸ¯ Ø¯Ù‚Øª 95%+**: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ 7 Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ\n",
        "- **âš¡ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§**: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯\n",
        "- **ğŸ§  ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø±**: Ø§Ù…Ù„Ø§ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ\n",
        "- **ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª**: Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ù‡Ø± ØµÙØ­Ù‡\n",
        "- **ğŸ”§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®ÙˆØ¯Ú©Ø§Ø±**: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ§ÛŒÙ„\n",
        "\n",
        "### ğŸ’¡ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:\n",
        "\n",
        "- Ø¨Ø±Ø§ÛŒ Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ØªØ±ØŒ PDF Ø¨Ø§ Ú©ÛŒÙÛŒØª 300+ DPI Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯\n",
        "- ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ù…ØªØ± Ø§Ø² 30 ØµÙØ­Ù‡ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯\n",
        "- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø³Øªâ€ŒÙ†ÙˆÛŒØ³ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ù‚Øª Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯\n",
        "\n",
        "### ğŸš€ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¬Ø¯Ø¯:\n",
        "\n",
        "Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯:\n",
        "1. **Runtime** â†’ **Restart session**\n",
        "2. Ù‡Ù…Ù‡ Ø³Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯\n",
        "3. ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯\n",
        "\n",
        "---\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "**ğŸ”¥ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ø§Ù…Ø¹Ù‡ ÙØ§Ø±Ø³ÛŒâ€ŒØ²Ø¨Ø§Ù† ğŸ”¥**\n",
        "\n",
        "*Ø¨Ù‡ØªØ±ÛŒÙ† OCR ÙØ§Ø±Ø³ÛŒØŒ Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ*\n",
        "\n",
        "â­ **Ø§Ú¯Ø± Ù…ÙÛŒØ¯ Ø¨ÙˆØ¯ØŒ Ø³ØªØ§Ø±Ù‡ ÙØ±Ø§Ù…ÙˆØ´ Ù†Ø´ÙˆØ¯!** â­\n",
        "\n",
        "</div>"
        "
